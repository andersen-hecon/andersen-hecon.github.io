[
  {
    "objectID": "teaching_resources/reading_in_data_in_R.html",
    "href": "teaching_resources/reading_in_data_in_R.html",
    "title": "Reading in data and basic data manipulation using R",
    "section": "",
    "text": "In this page we will get started using R to read in data, installing packages, and manipulating data.\nTo read in our data, we will use R’s tidyverse family of libraries. Before we begin, let’s make sure that we have the most recent version of tidyverse installed:\n\ninstall.packages(“tidyverse”)\n\nDepending on when you last updated your computer this may, or may not, result in a bunch of output about installing the package.\nNext, let’s load the library:\n\nlibrary(tidyverse)\n\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThe output that follows library(\"tidyverse\") is fairly typical and is R’s way of telling you that if you type in certain commands, such as filter, you may get the results of a different function than the one that you expected. The double colon notation (::) is a way to make sure that you got the version of the command that you wanted to use. So if you wanted to use the filter command from the stats package after this you would type stats::filter instead of just filter."
  },
  {
    "objectID": "teaching_resources/reading_in_data_in_R.html#getting-started",
    "href": "teaching_resources/reading_in_data_in_R.html#getting-started",
    "title": "Reading in data and basic data manipulation using R",
    "section": "",
    "text": "In this page we will get started using R to read in data, installing packages, and manipulating data.\nTo read in our data, we will use R’s tidyverse family of libraries. Before we begin, let’s make sure that we have the most recent version of tidyverse installed:\n\ninstall.packages(“tidyverse”)\n\nDepending on when you last updated your computer this may, or may not, result in a bunch of output about installing the package.\nNext, let’s load the library:\n\nlibrary(tidyverse)\n\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThe output that follows library(\"tidyverse\") is fairly typical and is R’s way of telling you that if you type in certain commands, such as filter, you may get the results of a different function than the one that you expected. The double colon notation (::) is a way to make sure that you got the version of the command that you wanted to use. So if you wanted to use the filter command from the stats package after this you would type stats::filter instead of just filter."
  },
  {
    "objectID": "teaching_resources/reading_in_data_in_R.html#reading-data-with-r",
    "href": "teaching_resources/reading_in_data_in_R.html#reading-data-with-r",
    "title": "Reading in data and basic data manipulation using R",
    "section": "Reading data with R",
    "text": "Reading data with R\nNow that we have loaded the tidyverse, let’s go get the data file. For this example, we are using a cleaned dataset that I have created from the National Health Interview Survey to demonstrate how insurance coverage changes over time. This file is located at https://andersen-hecon.github.io/data/nhis_for_example.csv and we can load the file (and assign it to the name data) using the code:\n\ndata&lt;-read_csv(“https://andersen-hecon.github.io/data/nhis_for_example.csv”)\n\nAs you can see below, executing this code results in R providing some data about the file–in particular the “Delimiter,” or how the columns of data are divided (commas in this case), and the types and names of the columns.\n\n\nRows: 4128 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): sex\ndbl (8): year, age, pop, any_insurance, private_insurance, chip_insurance, m...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe can suppress (or eliminate) this information using the option “show_col_types=FALSE” so that the code would read:\n\ndata&lt;-read_csv(“https://andersen-hecon.github.io/data/nhis_for_example.csv”, show_col_types=FALSE)\n\nNotice now that we have loaded the file that it appears in the environment window in the top right corner of the RStudio window:"
  },
  {
    "objectID": "teaching_resources/reading_in_data_in_R.html#summarizing-data",
    "href": "teaching_resources/reading_in_data_in_R.html#summarizing-data",
    "title": "Reading in data and basic data manipulation using R",
    "section": "Summarizing data",
    "text": "Summarizing data\nWe can look at the data we have loaded in several ways. The easiest (in RStudio) is to use the viewer either by clicking on the data object in the environment window or by typing View(data) in the console window. These ways of viewing the data provide a detailed, line by line, view of the data. We can achieve something similar by typing data or print(data) at the console:\n\n\n# A tibble: 4,128 × 9\n    year   age sex        pop any_insurance private_insurance chip_insurance\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n 1  2000     0 female 1950004         0.891             0.564        0.0147 \n 2  2000     0 male   1981206         0.886             0.617        0.00979\n 3  2000     1 female 2021247         0.860             0.536        0.0278 \n 4  2000     1 male   1971851         0.855             0.619        0.0147 \n 5  2000     2 female 1912194         0.883             0.697        0.0133 \n 6  2000     2 male   1900073         0.877             0.602        0.0126 \n 7  2000     3 female 1724788         0.886             0.632        0.00925\n 8  2000     3 male   2157487         0.898             0.624        0.0325 \n 9  2000     4 female 1979674         0.885             0.691        0.0172 \n10  2000     4 male   2010541         0.892             0.674        0.0233 \n# ℹ 4,118 more rows\n# ℹ 2 more variables: mcaid_insurance &lt;dbl&gt;, mcare_insurance &lt;dbl&gt;\n\n\nPlease pay careful attention to the last line of the output (“# ℹ 2 more variables: mcaid_insurance , mcare_insurance ”), which indicate that the screen wasn’t wide enough to show two of the variables (mcaid_insurace and mcare_insurance). Viewing the data allows us to see more variables and columns in RStudio:\n\nLastly, we can use the summary command to get some characteristics about the data and each variable by typing summary(data) at the console:\n\n\n      year           age           sex                 pop         \n Min.   :2000   Min.   : 0.0   Length:4128        Min.   : 192055  \n 1st Qu.:2006   1st Qu.:21.0   Class :character   1st Qu.:1609411  \n Median :2012   Median :42.5   Mode  :character   Median :1968180  \n Mean   :2012   Mean   :42.5                      Mean   :1777074  \n 3rd Qu.:2017   3rd Qu.:64.0                      3rd Qu.:2127850  \n Max.   :2023   Max.   :85.0                      Max.   :3702858  \n any_insurance    private_insurance chip_insurance     mcaid_insurance  \n Min.   :0.4579   Min.   :0.2905    Min.   :0.000000   Min.   :0.00000  \n 1st Qu.:0.8348   1st Qu.:0.5496    1st Qu.:0.000000   1st Qu.:0.05995  \n Median :0.8965   Median :0.6312    Median :0.000000   Median :0.09480  \n Mean   :0.8883   Mean   :0.6249    Mean   :0.010318   Mean   :0.12910  \n 3rd Qu.:0.9688   3rd Qu.:0.7040    3rd Qu.:0.005885   3rd Qu.:0.16583  \n Max.   :1.0000   Max.   :0.8603    Max.   :0.140555   Max.   :0.50108  \n mcare_insurance   \n Min.   :0.000000  \n 1st Qu.:0.004124  \n Median :0.024353  \n Mean   :0.250897  \n 3rd Qu.:0.159663  \n Max.   :1.000000  \n\n\nThe summary command provides useful information for datasets including the number of missing observations (in these data there are no missing observations) and measures of the distribution of each numerical variable. We can look at categorical variables using a variety of commands. One of the easiest is to count the levels of a variable, which reports the number of observations with each combination of values that you list in the count command:\nHere we count(data, sex), which lists the number of observations in data by sex.\n\n\n# A tibble: 2 × 2\n  sex        n\n  &lt;chr&gt;  &lt;int&gt;\n1 female  2064\n2 male    2064\n\n\nYou may be looking at the output above and wondering what is up with those values–do they make sense? In this case the answer is NO! When I created this dataset I aggregated an individual level dataset up to the age-by-sex-by-year, level. The (weighted) number of observations in each cell is in the variable pop in the dataset. As an exercise, use the help information for count (which you can get to by typing ?count in the console) to figure out how you might account for the pop in each cell. Here is my answer:\n\n\n# A tibble: 2 × 2\n  sex              n\n  &lt;chr&gt;        &lt;dbl&gt;\n1 female 3745890881.\n2 male   3589870752.\n\n\n\n\n\n\n\n\nTip\n\n\n\nR allows you to chain commands together using a “pipe” character (|&gt; that is | followed by &gt;). This allows you to rewrite things like count(data,sex) as data|&gt;count(sex). This feature allows you to break out the “nouns” in your process (i.e. data) from the verbs (i.e. count(sex)). I will use this notation below."
  },
  {
    "objectID": "teaching_resources/reading_in_data_in_R.html#the-core-dplyr-verbs",
    "href": "teaching_resources/reading_in_data_in_R.html#the-core-dplyr-verbs",
    "title": "Reading in data and basic data manipulation using R",
    "section": "The core dplyr verbs",
    "text": "The core dplyr verbs\nThere are five main dplyr “verbs”:\n\nselect, which allows you to select and reorder variables from a dataframe\nmutate, which allows you to create new or change existing variables (it is also possible to remove a variable).\nfilter, subsets a dataframe based on the condition(s) you provide in the filter statement\nsummarize aggregates a dataframe into groups that you define using either group_by() or the .by argument to summarize\narrange sorts and reorders a dataframe\n\n\nselect\nThe select command allows you to subset, reorder, and rename variables from a dataframe.\n\nSubsetting\nFor example, if we wanted to select only the variables year, sex, and age from the data dataframe we would type:\n\ndata|&gt;select(year,sex,age)\n\nThe output would look something like this:\n\n\n# A tibble: 4,128 × 3\n    year sex      age\n   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1  2000 female     0\n 2  2000 male       0\n 3  2000 female     1\n 4  2000 male       1\n 5  2000 female     2\n 6  2000 male       2\n 7  2000 female     3\n 8  2000 male       3\n 9  2000 female     4\n10  2000 male       4\n# ℹ 4,118 more rows\n\n\nI like to use the select command when I am writing code to help me make sure that I coded up a variable correctly, something that we will get to in the next command.\n\n\nReordering\nAnother use of the select command is to reorder the variables in a dataframe. For example, if we wanted to reorder the variables in the data dataframe so that age was the first variable, we would type:\n\ndata|&gt;select(age,everything())\n\nThe output would look something like this:\n\n\n# A tibble: 4,128 × 9\n     age  year sex        pop any_insurance private_insurance chip_insurance\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n 1     0  2000 female 1950004         0.891             0.564        0.0147 \n 2     0  2000 male   1981206         0.886             0.617        0.00979\n 3     1  2000 female 2021247         0.860             0.536        0.0278 \n 4     1  2000 male   1971851         0.855             0.619        0.0147 \n 5     2  2000 female 1912194         0.883             0.697        0.0133 \n 6     2  2000 male   1900073         0.877             0.602        0.0126 \n 7     3  2000 female 1724788         0.886             0.632        0.00925\n 8     3  2000 male   2157487         0.898             0.624        0.0325 \n 9     4  2000 female 1979674         0.885             0.691        0.0172 \n10     4  2000 male   2010541         0.892             0.674        0.0233 \n# ℹ 4,118 more rows\n# ℹ 2 more variables: mcaid_insurance &lt;dbl&gt;, mcare_insurance &lt;dbl&gt;\n\n\nNow age is the first column of the dataframe.\n\n\nRenaming\nFinally, we can use the select command to rename variables. For example, if we wanted to rename the variable year to yr we would type:\n\ndata|&gt;select(yr=year, everything())\n\nThe output would look something like this:\n\n\n# A tibble: 4,128 × 9\n      yr   age sex        pop any_insurance private_insurance chip_insurance\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n 1  2000     0 female 1950004         0.891             0.564        0.0147 \n 2  2000     0 male   1981206         0.886             0.617        0.00979\n 3  2000     1 female 2021247         0.860             0.536        0.0278 \n 4  2000     1 male   1971851         0.855             0.619        0.0147 \n 5  2000     2 female 1912194         0.883             0.697        0.0133 \n 6  2000     2 male   1900073         0.877             0.602        0.0126 \n 7  2000     3 female 1724788         0.886             0.632        0.00925\n 8  2000     3 male   2157487         0.898             0.624        0.0325 \n 9  2000     4 female 1979674         0.885             0.691        0.0172 \n10  2000     4 male   2010541         0.892             0.674        0.0233 \n# ℹ 4,118 more rows\n# ℹ 2 more variables: mcaid_insurance &lt;dbl&gt;, mcare_insurance &lt;dbl&gt;\n\n\n\n\n\nmutate\nThe mutate command can be used to create new variables by “assigning” a value to a variable name. For example, we can create a new variable age_squared that is the square of age by typing:\n\ndata|&gt;mutate(age_squared=age^2)\n\nIf you run that line of code in your console window you should see something like:\n\n\n# A tibble: 4,128 × 10\n    year   age sex        pop any_insurance private_insurance chip_insurance\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n 1  2000     0 female 1950004         0.891             0.564        0.0147 \n 2  2000     0 male   1981206         0.886             0.617        0.00979\n 3  2000     1 female 2021247         0.860             0.536        0.0278 \n 4  2000     1 male   1971851         0.855             0.619        0.0147 \n 5  2000     2 female 1912194         0.883             0.697        0.0133 \n 6  2000     2 male   1900073         0.877             0.602        0.0126 \n 7  2000     3 female 1724788         0.886             0.632        0.00925\n 8  2000     3 male   2157487         0.898             0.624        0.0325 \n 9  2000     4 female 1979674         0.885             0.691        0.0172 \n10  2000     4 male   2010541         0.892             0.674        0.0233 \n# ℹ 4,118 more rows\n# ℹ 3 more variables: mcaid_insurance &lt;dbl&gt;, mcare_insurance &lt;dbl&gt;,\n#   age_squared &lt;dbl&gt;\n\n\nNote that we have now created a new column, but we can only tell by looking at the bottom line of the output. Let’s fix that by using the select command:\n\ndata|&gt;mutate(age_squared=age^2)|&gt;select(age, age_squared,everything())\n\n\n\n# A tibble: 4,128 × 10\n     age age_squared  year sex        pop any_insurance private_insurance\n   &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n 1     0           0  2000 female 1950004         0.891             0.564\n 2     0           0  2000 male   1981206         0.886             0.617\n 3     1           1  2000 female 2021247         0.860             0.536\n 4     1           1  2000 male   1971851         0.855             0.619\n 5     2           4  2000 female 1912194         0.883             0.697\n 6     2           4  2000 male   1900073         0.877             0.602\n 7     3           9  2000 female 1724788         0.886             0.632\n 8     3           9  2000 male   2157487         0.898             0.624\n 9     4          16  2000 female 1979674         0.885             0.691\n10     4          16  2000 male   2010541         0.892             0.674\n# ℹ 4,118 more rows\n# ℹ 3 more variables: chip_insurance &lt;dbl&gt;, mcaid_insurance &lt;dbl&gt;,\n#   mcare_insurance &lt;dbl&gt;\n\n\nNow, take a look at the output. Is it correct? Is the second column the square of the first column? If not, what went wrong?\nWe will use the mutate command to create a new variable that creates two variables that we will use later. First, we will create a variable over65 that equals 1 if age is greater than or equal to 65. In order to do this, we will use the if_else command, which is a way to create a conditional variable. The if_else command takes three arguments: the condition, the value if the condition is true, and the value if the condition is false. For example, if we wanted to create a variable over65 that equals 1 if age is greater than or equal to 65 and 0 otherwise we would type:\n\ndata|&gt;mutate(over65=if_else(age&gt;=65,1,0))\n\n\n\n# A tibble: 4,128 × 10\n     age over65  year sex        pop any_insurance private_insurance\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n 1     0      0  2000 female 1950004         0.891             0.564\n 2     0      0  2000 male   1981206         0.886             0.617\n 3     1      0  2000 female 2021247         0.860             0.536\n 4     1      0  2000 male   1971851         0.855             0.619\n 5     2      0  2000 female 1912194         0.883             0.697\n 6     2      0  2000 male   1900073         0.877             0.602\n 7     3      0  2000 female 1724788         0.886             0.632\n 8     3      0  2000 male   2157487         0.898             0.624\n 9     4      0  2000 female 1979674         0.885             0.691\n10     4      0  2000 male   2010541         0.892             0.674\n# ℹ 4,118 more rows\n# ℹ 3 more variables: chip_insurance &lt;dbl&gt;, mcaid_insurance &lt;dbl&gt;,\n#   mcare_insurance &lt;dbl&gt;\n\n\nLet’s look at over65 using count too:\n\ncount(data,over65)\n\n\n\nError in `count()`:\n! Must group by variables found in `.data`.\n✖ Column `over65` is not found.\n\n\nWait! We got an error! Why? Because over65 isn’t there! What happened?\nR doesn’t update a dataframe with the calculations that you did, unless you explicitly tell it to do so. How do we do that? We can use the = or &lt;-. The differences between these two are a bit technical and not worth diving into. For now, let’s run that command again, but this time assign the results to data:\n\ndata&lt;-data|&gt;mutate(over65=if_else(age&gt;=65,1,0))\n\n\n\n# A tibble: 2 × 2\n  over65     n\n   &lt;dbl&gt; &lt;int&gt;\n1      0  3120\n2      1  1008\n\n\nNow, let’s create one more variable age_minus_65 which, as the name suggests, is simply age minus 65:\n\ndata&lt;-data|&gt;mutate(age_minus_65=age-65)\n\nWe can check if we got it right by counting the age and age_minus_65:\n\ndata|&gt;count(age,age_minus_65)\n\n\n\n# A tibble: 86 × 3\n     age age_minus_65     n\n   &lt;dbl&gt;        &lt;dbl&gt; &lt;int&gt;\n 1     0          -65    48\n 2     1          -64    48\n 3     2          -63    48\n 4     3          -62    48\n 5     4          -61    48\n 6     5          -60    48\n 7     6          -59    48\n 8     7          -58    48\n 9     8          -57    48\n10     9          -56    48\n# ℹ 76 more rows\n\n\n\n\nfilter\nThe filter command allows you to subset the rows of your data (not columns, which is what select does) using a conditional expression. For example, if we wanted to select only the rows of the data dataframe where age is greater than or equal to 65 we would type:\n\ndata|&gt;filter(age&gt;=65)\n\n\n\n# A tibble: 1,008 × 11\n    year   age sex        pop any_insurance private_insurance chip_insurance\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n 1  2000    65 female 1132678         0.982             0.620              0\n 2  2000    65 male    937803         0.978             0.725              0\n 3  2000    66 female  872208         0.997             0.603              0\n 4  2000    66 male    784400         0.954             0.625              0\n 5  2000    67 female  993634         0.967             0.630              0\n 6  2000    67 male    893301         0.955             0.666              0\n 7  2000    68 female 1079059         0.993             0.635              0\n 8  2000    68 male    733120         0.957             0.646              0\n 9  2000    69 female  968159         0.991             0.692              0\n10  2000    69 male    681218         1                 0.699              0\n# ℹ 998 more rows\n# ℹ 4 more variables: mcaid_insurance &lt;dbl&gt;, mcare_insurance &lt;dbl&gt;,\n#   over65 &lt;dbl&gt;, age_minus_65 &lt;dbl&gt;\n\n\nIn this output, the first value of age that we see is 65, which is what we would expect.\nYou can also do more complicated expressions. For example, if we wanted to select only the rows of the data dataframe where age is greater than or equal to 65 and sex is equal to 1 we would type:\n\ndata|&gt;filter(age&gt;=65, sex==1)\n\n\n\n# A tibble: 0 × 11\n# ℹ 11 variables: year &lt;dbl&gt;, age &lt;dbl&gt;, sex &lt;chr&gt;, pop &lt;dbl&gt;,\n#   any_insurance &lt;dbl&gt;, private_insurance &lt;dbl&gt;, chip_insurance &lt;dbl&gt;,\n#   mcaid_insurance &lt;dbl&gt;, mcare_insurance &lt;dbl&gt;, over65 &lt;dbl&gt;,\n#   age_minus_65 &lt;dbl&gt;\n\n\n\n\n\n\n\n\nTip: Logical comparisons in R\n\n\n\n\n\n\nSymbol\nDescription\nExample\n\n\n\n\n&gt;\nGreater than\n5 &gt; 6 returns FALSE\n\n\n&lt;\nLess than\n5 &lt; 6 returns TRUE\n\n\n==\nEquals to\n10 == 10 returns TRUE\n\n\n!=\nNot equal to\n10 != 10 returns FALSE\n\n\n&gt;=\nGreater than or equal to\n5 &gt;= 6 returns FALSE\n\n\n&lt;=\nLess than or equal to\n6 &lt;= 6 returns TRUE\n\n\n\n\n\nWhen using the filter command, you can put in multiple statements (like we did to filter by age AND sex). But, when you do that the filter command only keeps rows that meet all of those criteria. If you want to keep rows that meet any of the criteria you can use the | symbol. For example, if we wanted to select only the rows of the data dataframe where age is greater than or equal to 65 OR sex is equal to 1 we would type:\n\ndata|&gt;filter(age&gt;=65 | sex==1)\n\n\n\n# A tibble: 1,008 × 11\n    year   age sex        pop any_insurance private_insurance chip_insurance\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n 1  2000    65 female 1132678         0.982             0.620              0\n 2  2000    65 male    937803         0.978             0.725              0\n 3  2000    66 female  872208         0.997             0.603              0\n 4  2000    66 male    784400         0.954             0.625              0\n 5  2000    67 female  993634         0.967             0.630              0\n 6  2000    67 male    893301         0.955             0.666              0\n 7  2000    68 female 1079059         0.993             0.635              0\n 8  2000    68 male    733120         0.957             0.646              0\n 9  2000    69 female  968159         0.991             0.692              0\n10  2000    69 male    681218         1                 0.699              0\n# ℹ 998 more rows\n# ℹ 4 more variables: mcaid_insurance &lt;dbl&gt;, mcare_insurance &lt;dbl&gt;,\n#   over65 &lt;dbl&gt;, age_minus_65 &lt;dbl&gt;\n\n\nThink about how you would check if you got the right answer. What would you do?\n\n\nsummarize\nSometimes you want to summarize or aggregate your data, either to simplify your analysis later on, make a dataset that is easier for students to use, or other reasons. You do that with the summarize verb in R. For example, if we wanted to summarize the data dataframe by year and sex we would type: &gt; data|&gt;group_by(year,sex)|&gt;summarize(mean_age=mean(age),n=n())\n\ndata|&gt;group_by(year,sex)|&gt;summarize(mean_age=mean(age),n=n())\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 48 × 4\n# Groups:   year [24]\n    year sex    mean_age     n\n   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt;\n 1  2000 female     42.5    86\n 2  2000 male       42.5    86\n 3  2001 female     42.5    86\n 4  2001 male       42.5    86\n 5  2002 female     42.5    86\n 6  2002 male       42.5    86\n 7  2003 female     42.5    86\n 8  2003 male       42.5    86\n 9  2004 female     42.5    86\n10  2004 male       42.5    86\n# ℹ 38 more rows\n\n\nHmmm…. this doesn’t look right, does it? Every combination appears to have the same mean_age and n. What went wrong? The problem is that we didn’t tell R to calculate the mean and count taking into account the number of observations wihtin each cell. We can do that in one of two ways. We can use the weighted.mean command to compute the weighted mean (and then use the sum command to get the sum of pop). But we can also go back to “first principles” and remember that the mean of a variable is the sum of the variable times the weight divided by the sum of the weight.\nSo, we can calculate the mean of age by year and sex by typing:\n\ndata|&gt;group_by(year,sex)|&gt;summarize( mean_age=sum(age*pop)/sum(pop), n=sum(pop))\n\n\ndata|&gt;group_by(year,sex)|&gt;summarize(\n  mean_age=sum(age*pop)/sum(pop),\n n=sum(pop))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 48 × 4\n# Groups:   year [24]\n    year sex    mean_age         n\n   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1  2000 female     36.3 140398513\n 2  2000 male       34.3 133625780\n 3  2001 female     36.5 141586626\n 4  2001 male       34.4 134894673\n 5  2002 female     36.6 142734367\n 6  2002 male       34.6 136060670\n 7  2003 female     37.0 146414283\n 8  2003 male       34.8 139600816\n 9  2004 female     37.2 147355026\n10  2004 male       35.0 140902784\n# ℹ 38 more rows\n\n\nThis looks better. There is now some variation in the average age and the group sizes. In the next page, we will dive into this a bit more carefully when we introduce ggplot as a means to visualize data.\nWait a minute, what is gorup_by doing? group_by is a little helper verb that makes everything that follows run at the level of each individual group. For mutate this isn’t (usually) a big deal, but for summarize it is vital because that is how we tell summarize what groups to use for summarizeing the data.\n\n\n\n\n\n\nTip: group_by and .by\n\n\n\nMost tidyverse commands that work with groups have a .by argument that allows you to specify the groups that you want to use. For example, you could have written the summarize command above as: &gt; data|&gt;summarize( &gt; mean_age=sum(age*pop)/sum(pop), &gt; n=sum(pop), .by=c(year,sex))\nWhere the final c function tells R to use BOTH year and sex together to construct the groups.\n\n\n\nSummarizing multiple variables at once\nSuppose we wanted to compute the weighted average of a few more variables (not just age). Maybe we wanted to look at changes in isnurance coverage by year (we will ignore sex for the moment). We can use the across command to help us out. across takes a list of variable names and a function template to do the same computation across the columns. To calculate the average age, the average of mcaid_insurance, the average of pricate_insruance. and the average of mcare_insurance by year we would type:\n\ndata|&gt;group_by(year)|&gt; summarize( across( c(age,mcaid_insurance,private_insurance,mcare_insurance), ~sum(.x*pop)/sum(pop) ) )\n\n\ndata|&gt;\n  group_by(year)|&gt;\n  summarize(across(c(age,mcaid_insurance,private_insurance,mcare_insurance),~sum(.x*pop)/sum(pop)))\n\n# A tibble: 24 × 5\n    year   age mcaid_insurance private_insurance mcare_insurance\n   &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;             &lt;dbl&gt;           &lt;dbl&gt;\n 1  2000  35.3          0.0810             0.708           0.125\n 2  2001  35.5          0.0806             0.708           0.125\n 3  2002  35.6          0.0928             0.688           0.124\n 4  2003  36.0          0.0947             0.674           0.125\n 5  2004  36.1          0.0963             0.672           0.127\n 6  2005  36.3          0.102              0.663           0.131\n 7  2006  36.4          0.110              0.644           0.134\n 8  2007  36.5          0.111              0.648           0.136\n 9  2008  36.7          0.115              0.639           0.141\n10  2009  36.9          0.124              0.617           0.141\n# ℹ 14 more rows\n\n\nNow we see that we have the columns that we want!\n\n\n\narrange\nThe final verb we will cover is arrange, which is used to arrange or order a dataframe. For example, if we wanted to order the data dataframe by age we would type:\n\ndata|&gt;arrange(year)\n\n\n\n# A tibble: 4,128 × 11\n    year   age sex        pop any_insurance private_insurance chip_insurance\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n 1  2000     0 female 1950004         0.891             0.564        0.0147 \n 2  2000     0 male   1981206         0.886             0.617        0.00979\n 3  2001     0 female 1945606         0.942             0.675        0.0239 \n 4  2001     0 male   1995217         0.921             0.626        0.0225 \n 5  2002     0 female 1931185         0.938             0.579        0.0343 \n 6  2002     0 male   2010451         0.926             0.554        0.0469 \n 7  2003     0 female 1849941         0.936             0.540        0.0118 \n 8  2003     0 male   2020784         0.942             0.517        0.0273 \n 9  2004     0 female 1923205         0.943             0.500        0.0212 \n10  2004     0 male   1968188         0.940             0.551        0.0442 \n# ℹ 4,118 more rows\n# ℹ 4 more variables: mcaid_insurance &lt;dbl&gt;, mcare_insurance &lt;dbl&gt;,\n#   over65 &lt;dbl&gt;, age_minus_65 &lt;dbl&gt;\n\n\nNow we see the values for age 0 by year and then if you went down the table you would see the values for age 1 by year, and so on.\nWe can even arrange by more than one variable by simply adding another variable to the list:\n\ndata|&gt;arrange(year,any_insurance)\n\n\n\n# A tibble: 4,128 × 11\n    year   age sex         pop any_insurance private_insurance chip_insurance\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n 1  2000     0 male   1981206          0.886             0.617        0.00979\n 2  2000     0 female 1950004          0.891             0.564        0.0147 \n 3  2019     0 female 1877997.         0.914             0.523        0.0461 \n 4  2001     0 male   1995217          0.921             0.626        0.0225 \n 5  2002     0 male   2010451          0.926             0.554        0.0469 \n 6  2007     0 female 2062998          0.929             0.437        0.0541 \n 7  2012     0 female 1879587          0.930             0.371        0.0496 \n 8  2010     0 male   1974740          0.934             0.505        0.0358 \n 9  2008     0 male   2052277          0.934             0.478        0.0484 \n10  2006     0 male   2085037          0.936             0.473        0.0389 \n# ℹ 4,118 more rows\n# ℹ 4 more variables: mcaid_insurance &lt;dbl&gt;, mcare_insurance &lt;dbl&gt;,\n#   over65 &lt;dbl&gt;, age_minus_65 &lt;dbl&gt;\n\n\nNow the data are sorted by age and then the fraction with any private insurance. But what if we wanted to see the year when each age group had the highest level of insurance? Then we need to switch from the default “ascending” sort order to a “descending” sort order, which we can do with the desc operator:\n\ndata|&gt;arrange(year,desc(any_insurance))\n\n\n\n# A tibble: 4,128 × 11\n    year   age sex         pop any_insurance private_insurance chip_insurance\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n 1  2020     0 female 2201385.         0.992             0.423        0.0247 \n 2  2011     0 male   2052705          0.988             0.444        0.0697 \n 3  2018     0 male   1986534          0.985             0.543        0.0535 \n 4  2017     0 female 1767121          0.984             0.500        0.0367 \n 5  2018     0 female 1940610          0.983             0.565        0.00631\n 6  2023     0 male   1882530          0.983             0.585        0.0204 \n 7  2016     0 female 1886954          0.978             0.593        0.0113 \n 8  2016     0 male   1930821          0.977             0.504        0.0265 \n 9  2015     0 female 1964199          0.977             0.472        0.0393 \n10  2011     0 female 1868103          0.976             0.487        0.0429 \n# ℹ 4,118 more rows\n# ℹ 4 more variables: mcaid_insurance &lt;dbl&gt;, mcare_insurance &lt;dbl&gt;,\n#   over65 &lt;dbl&gt;, age_minus_65 &lt;dbl&gt;\n\n\nNow we have any_insurance in a decreasing order within each of the age groups."
  },
  {
    "objectID": "teaching_resources/reading_in_data_in_R.html#saving-your-work",
    "href": "teaching_resources/reading_in_data_in_R.html#saving-your-work",
    "title": "Reading in data and basic data manipulation using R",
    "section": "Saving your work",
    "text": "Saving your work\nBefore we leave this page, we should save our work. We will use the write_csv command to do this. Before you save the file though, you should check where it will be saved. This is called your “working directory” and you can see what it is using getwd(). Type it in the console and you should see a location on your hard drive. Does this location seem right for you? If not, you can change it using the setwd command.\nNow that you have chosen the folder, let’s save the file:\n\ndata|&gt;write_csv(“nhis_working_data.csv”)"
  },
  {
    "objectID": "teaching_resources/reading_in_data_in_R.html#practice",
    "href": "teaching_resources/reading_in_data_in_R.html#practice",
    "title": "Reading in data and basic data manipulation using R",
    "section": "Practice",
    "text": "Practice\nHere are some practice exercises that you can do with the data dataframe and the core dplyr verbs.\n\nCalculate average insurance coverage (for each type) using the over65 variable that we created earlier by year.\n\n\n\n\n\n\n\nMy result\n\n\n\n\n\nTo highlight an event of particular interest, I am going to show results for the years 2012 through 2016\n\n\n`summarise()` has grouped output by 'over65'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 10 × 7\n# Groups:   over65 [2]\n   over65  year   age any_insurance mcaid_insurance private_insurance\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt;             &lt;dbl&gt;\n 1      0  2012  31.8         0.831          0.148              0.616\n 2      0  2013  31.9         0.832          0.149              0.613\n 3      0  2014  32.0         0.862          0.171              0.629\n 4      0  2015  32.0         0.889          0.185              0.649\n 5      0  2016  32.1         0.895          0.196              0.650\n 6      1  2012  74.0         0.992          0.0680             0.516\n 7      1  2013  73.9         0.989          0.0734             0.497\n 8      1  2014  73.8         0.991          0.0744             0.501\n 9      1  2015  73.8         0.994          0.0789             0.488\n10      1  2016  73.7         0.991          0.0776             0.482\n# ℹ 1 more variable: mcare_insurance &lt;dbl&gt;\n\n\n\n\n\n\nCalculate the number of people with any insurance by year\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou will need to do a calculation along the way. You can either do that in a mutate step or in the summarize step. I would recommend the mutate step.\n\n\n\n\n\n\n\n\n\nMy result\n\n\n\n\n\n\n\n# A tibble: 24 × 2\n    year number_insured\n   &lt;dbl&gt;          &lt;dbl&gt;\n 1  2000      232859140\n 2  2001      237860025\n 3  2002      238411316\n 4  2003      242788085\n 5  2004      245359798\n 6  2005      247344515\n 7  2006      247511543\n 8  2007      252161957\n 9  2008      254112178\n10  2009      253840146\n# ℹ 14 more rows\n\n\n\n\n\n\nCreate a new variable post_ACA that equals 1 if the year if greater than or equal 2014 and 0 otherwise. Then calculate the average of any_insurance in each of the cells defined by post_ACA, over65 and sex.\n\n\n\n\n\n\n\nMy result\n\n\n\n\n\n\n\n`summarise()` has grouped output by 'post_ACA', 'over65'. You can override\nusing the `.groups` argument.\n\n\n# A tibble: 8 × 4\n# Groups:   post_ACA, over65 [4]\n  post_ACA over65 sex    any_insurance\n     &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;\n1        0      0 female         0.843\n2        0      0 male           0.817\n3        0      1 female         0.990\n4        0      1 male           0.989\n5        1      0 female         0.899\n6        1      0 male           0.876\n7        1      1 female         0.992\n8        1      1 male           0.992"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "I am Martin Sparre Andersen, an Associate Professor in the Department of Economics at the Bryan School of Business and Economics at the University of North Carolina at Greensboro. I hold a BA in Mathematics from Columbia University, an MPH in Health Policy from Yale University, and a PhD in Health Policy with a concentration in Economics from Harvard University. After completing my doctoral studies, I pursued a one-year postdoctoral fellowship at the National Bureau for Economic Research.\nMy primary research interests lie in the fields of health and education economics, with a particular focus on utilizing human mobility data. To date, I have been involved in projects on cancer care, health insurance, prescription drug benefit design, and reproductive health care. I am also active in, essentially, epidemiological research, trying to infer disease trends from mobility data.\nYou can book a meeting with me here\n\nDepartment of Economics\nUniversity of North Carolina Greensboro\nBryan Building 448\nPO Box 26165\nGreensboro, NC 27402-6165\nmsander4@uncg.edu"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am Martin Sparre Andersen, an Associate Professor in the Department of Economics at the Bryan School of Business and Economics at the University of North Carolina at Greensboro. I hold a BA in Mathematics from Columbia University, an MPH in Health Policy from Yale University, and a PhD in Health Policy with a concentration in Economics from Harvard University. After completing my doctoral studies, I pursued a one-year postdoctoral fellowship at the National Bureau for Economic Research.\nI chose to study health policy after a stint as a healthcare investment banker, where I became acutely aware of the deficiencies in the U.S. healthcare system. Driven by a desire to effect change rather than merely complain about these issues, I embarked on a path to learn more about health policy and ultimately found my passion in health economics.\nMy primary research interests lie in the fields of health and education economics, with a particular focus on utilizing human mobility data to study various topics. \nIn addition to my research, I am deeply committed to teaching. I take great pride in teaching Principles of Microeconomics, a course that is often one of the first college experiences for many of my students, who are frequently first-generation college students. It is incredibly rewarding for me, and I am honored to play a role in their educational journey.\nOutside of my professional life, I enjoy cooking and baking, with a particular talent for making macarons. I also love reading and spending time playing with my children."
  },
  {
    "objectID": "Teaching.html",
    "href": "Teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "I have taught a variety of courses during my time at UNCG. Currently I regularly teach ECO 637, Empirical Health Economics, ECO 498, Capstone Seminar in Economics, and ECO 201, Principles of Microeconomics. I have previously taught Health Economics at both the undergraduate and doctoral level and microeconomic theory at the master’s level."
  },
  {
    "objectID": "Teaching.html#using-ai-in-teaching",
    "href": "Teaching.html#using-ai-in-teaching",
    "title": "Teaching",
    "section": "Using AI in teaching",
    "text": "Using AI in teaching\nI have become a big fan of using large language models to support my teaching. Currently, I have several ChatGPT bots that I have setup with course materials, or the closest I could get that I could feed into the chatbot, so that I can use the bot to help me with material development."
  },
  {
    "objectID": "Research.html",
    "href": "Research.html",
    "title": "Research",
    "section": "",
    "text": "I am a health economist with an interest and, recently, a focus on applied research using human mobility data derived from smartphones.\n\nPublished papers\n\nDoes paid sick leave encourage staying at home? Evidence from the United States during a pandemic (2023)\nAuthors: Martin Andersen, Johanna Catherine Maclean, Michael F. Pesko, Kosali Simon\nRead the paper\n\n\n\nAbstract\n\nWe study the impact of a temporary U.S. paid sick leave mandate that became effective April 1st, 2020 on self-quarantining, proxied by physical mobility behaviors gleaned from cellular devices. We study this policy using generalized difference-in-differences methods, leveraging pre-policy county-level heterogeneity in the share of workers likely eligible for paid sick leave benefits. We find that the policy leads to increased self-quarantining as proxied by staying home. We also find that COVID-19 confirmed cases decline post-policy.\n\n\n\nTexas Senate Bill 8 significantly reduced travel to abortion clinics in Texas (2023)\nAuthors: Martin S. Andersen, Christopher Marsicano, Mayra Pineda Torres, David Slusky\nRead the paper\n\n\n\nAbstract\n\nThe Dobbs v. Jackson decision by the United States Supreme Court has rescinded the constitutional guarantee of abortion across the United States. As a result, at least 13 states have banned abortion access with unknown effects. Using “Texas” SB8 law that similarly restricted abortions in Texas, we provide insight into how individuals respond to these restrictions using aggregated and anonymized human mobility data. We find that “Texas” SB 8 law reduced mobility near abortion clinics in Texas by people who live in Texas and those who live outside the state. We also find that mobility from Texas to abortion clinics in other states increased, with notable increases in Missouri and Arkansas, two states that subsequently enacted post-Dobbs bans. These results highlight the importance of out-of-state abortion services for women living in highly restrictive states.\n\n\n\nA prospective examination of health care costs associated with posttraumatic stress disorder diagnostic status and symptom severity among veterans (2022)\nAuthors: Kelly L. Harper, Samantha Moshier, Stephanie Ellickson-Larew, Martin S. Andersen, Blair E. Wisco, Colin T. Mahoney, Terence M. Keane, Brian P. Marx\nRead the paper\n\n\n\nAbstract\n\nPosttraumatic stress disorder (PTSD) is associated with increased health care costs; however, most studies exploring this association use PTSD diagnostic data in administrative records, which can contain inaccurate diagnostic information and be confounded by the quantity of service use. We used a diagnostic interview to determine PTSD diagnostic status and examined associations between PTSD symptom severity and health care costs and utilization, extracted from Veteran Health Administration (VHA) administrative databases. Using a nationwide longitudinal sample of U.S. veterans with and without PTSD (N = 1,377) enrolled in VHA health care, we determined the costs and utilization of mental health and non–mental health outpatient, pharmacy, and inpatient services for 1 year following cohort enrollment. Relative to veterans without PTSD, those with PTSD had higher total health care\n\n\n\nUtilization Management in the Medicare Part D Program and Prescription Drug Utilization (2022)\nAuthors: Martin S. Andersen\nRead the paper\n\n\n\nAbstract\n\nMedicare Part D has significantly enhanced access to prescription drugs among Medicare beneficiaries. However, the recent rapid rise of utilization management policies in the Medicare Part D program may have adversely affected access to prescription drugs. I study the effects of expected and observed exposure to utilization management in prescription drug utilization using Medicare Part D claims data from 2009 to 2016 and an instrumental variables strategy based on the interaction of lagged health status and the set of plans available to each beneficiary. I find that the expected share of spending subject to utilization management increases the observed share, with the smallest effect for prior authorization. Increases in the expected share of drug spending subject to prior authorization increases Part D spending by $122.27 per percentage point, with almost three-quarters of this increase being paid by the Medicare program, rather than beneficiaries or plans. Comparable increases in step therapy and quantity limit exposure increase spending by $46 and decrease spending by $31, respectively. Interestingly, increased exposure to prior authorization and quantity limits increases the average price per 30-day prescription.\n\n\n\nEffects of utilization management on health outcomes: evidence from urinary tract infections and community-acquired pneumonia (2022)\nAuthors: Martin Andersen, Anurag Pant\nRead the paper\n\n\n\nAbstract\n\nBackground Utilization management policies are pervasive in the Medicare Part D program. We assess the effect of utilization management restrictions in the Medicare Part D program on the quality of care in two clinical areas – community-acquired pneumonia (CAP) and urinary tract infections (UTI).Methods In this study, we identified new cases of CAP and UTI from Medicare claims data from 2010 to 2016. We assessed the relationship between exposure to utilization management for antibiotic medications suitable for treating these conditions and adverse health outcomes, based on the Agency for Healthcare Research and Quality prevention quality indicators.Results We identified 147,526 cases of CAP and 632,407 UTI cases in our data. In these samples, the adverse event rate varied from 3.6 to 5.7%. The probability of an adverse event increased by 0.75 (p = 0.061) percentage points for each ten percentage point increase in exposure to quantity limits (one form of utilization management) among people with CAP. There was no relationship between utilization management and adverse events in the UTI cohort.Conclusions In some circumstances, exposure to utilization management policies–particularly quantity limits–may adversely affect health.\n\n\n\nAssociation of opioid utilization management with prescribing and overdose (2022)\nAuthors: Martin S. Andersen, Vincent Lorenz, Anurag Pant, Jeremy W. Bray, G. Caleb Alexander\nRead the paper\n\n\n\nAbstract\n\nOBJECTIVES: Deaths from prescription opioids have reached epidemic levels in the United States, yet little is known about how insurers’ coverage policies may affect rates of fatal and nonfatal overdose among individuals filling an opioid prescription. STUDY DESIGN: Retrospective cohort study using 2010-2016 Medicare claims data for beneficiaries with 1 or more filled prescriptions for a Schedule II opioid. METHODS: Outcomes were opioid volume dispensed in morphine milligram equivalents (MME), number of days supplied, and number of pills dispensed on each prescription and emergency department or inpatient stay associated with an opioid overdose during a prescription or within 7 days of the end of the prescription. RESULTS: A total of 7.03 million prescriptions for Schedule II opioids were dispensed over 1.87 million Part D beneficiary-years. The 7.03 million opioid prescriptions were associated with 8.5 opioid overdoses per 10,000 prescriptions. Prior authorization was associated with larger opioid volumes per prescription (103.6 MME; 95% CI, 36.2-171.0). Step therapy was associated with a greater number of days supplied (0.62 days; 95% CI, 0.10-1.13) and more pills dispensed (6.12 pills; 95% CI, 2.17-10.1). Quantity limits were associated with smaller opioid volumes (24.3 MME; 95% CI, 12.3-36.3) and fewer pills dispensed (2.35 pills; 95% CI, 1.77-2.93). In adjusted models, beneficiaries filling an opioid requiring prior authorization experienced 3.3 fewer overdoses per 10,000 prescriptions (95% CI, 0.41-6.2). CONCLUSIONS: Opioid utilization management among these beneficiaries was associated with mixed effects on opioid prescribing, and prior authorization was associated with a decreased likelihood of subsequent overdose. Further work exploring the impact of utilization management and insurer policies is needed.\n\n\n\nCollege openings in the United States increase mobility and COVID-19 incidence (2022)\nAuthors: Martin Andersen, Ana I. Bento, Anirban Basu, Christopher R. Marsicano, Kosali I. Simon\nRead the paper\n\n\n\nAbstract\n\nSchool and college reopening-closure policies are considered one of the most promising non-pharmaceutical interventions for mitigating infectious diseases. Nonetheless, the effectiveness of these policies is still debated, largely due to the lack of empirical evidence on behavior during implementation. We examined U.S. college reopenings’ association with changes in human mobility within campuses and in COVID-19 incidence in the counties of the campuses over a twenty-week period around college reopenings in the Fall of 2020. We used an integrative framework, with a difference-in-differences design comparing areas with a college campus, before and after reopening, to areas without a campus and a Bayesian approach to estimate the daily reproductive number (Rt). We found that college reopenings were associated with increased campus mobility, and increased COVID-19 incidence by 4.9 cases per 100,000 (95% confidence interval [CI]: 2.9–6.9), or a 37% increase relative to the pre-period mean. This reflected our estimate of increased transmission locally after reopening. A greater increase in county COVID-19 incidence resulted from campuses that drew students from counties with high COVID-19 incidence in the weeks before reopening (χ2(2) = 8.9\n\n\n\nImpacts of state COVID-19 reopening policy on human mobility and mixing behavior (2021)\nAuthors: Thuy D. Nguyen, Sumedha Gupta, Martin Andersen, Ana I. Bento, Kosali I. Simon, Coady Wing\nRead the paper\n\n\n\nAbstract\n\nThis study quantifies the effect of the 2020 state COVID economic activity reopening policies on daily mobility and mixing behavior, adding to the economic literature on individual responses to public health policy that addresses public contagion risks. We harness cellular device signal data and the timing of reopening plans to provide an assessment of the extent to which human mobility and physical proximity in the United States respond to the reversal of state closure policies. We observe substantial increases in mixing activities, 13.56% at 4 days and 48.65% at 4 weeks, following reopening events. Echoing a theme from the literature on the 2020 closures, mobility outside the home increased on average prior to these state actions. Furthermore, the largest increases in mobility occurred in states that were early adopters of closure measures and hard-hit by the pandemic, suggesting that psychological fatigue is an important barrier to implementation of closure policies extending for prolonged periods of time.\n\n\n\nRequiring Versus Recommending Preparation Before Class: Does It Matter? (2018)\nAuthors: Martin S. Andersen, Dora Gicheva, Jeffrey Sarbaum\nRead the paper\n\n\n\nAbstract\n\nAsking students to come to class prepared is quite common in undergraduate and graduate education. We use a quasiexperimental design to assess whether requiring undergraduate students in an introductory course to review prior to lecture the material that will be taught in class enhances their understanding of key concepts. We find that requiring rather than recommending preparation before class increases exam scores by about a quarter of a standard deviation, or roughly a third of a letter grade, for students in the second and third quartiles of the ability distribution but has little impact on very high- or low-ability students. We also estimate local average treatment effects, from which we draw a similar conclusion: reviewing the material before lecture benefits students in the middle of the ability distribution but has essentially no impact on the top and bottom quartiles.\n\n\n\nEffects of Medicare coverage for the chronically ill on health insurance, utilization, and mortality: Evidence from coverage expansions affecting people with end-stage renal disease (2018)\nAuthors: Martin S. Andersen\nRead the paper\n\n\n\nAbstract\n\nI study the effect of the 1973 expansions of Medicare coverage among individuals with end-stage renal disease (ESRD) on insurance coverage, health care utilization, and mortality. I find that the expansions increased insurance coverage by between 22 and 30 percentage points, in models that include trends in age, with the increase explained by Medicare coverage, and increased physician visits by 25–35 percent. These expansions also decreased mortality due to kidney disease in the under 65 population by between 0.5 and 1.0 deaths per 100,000. Lastly, I provide evidence for two mechanisms that affected mortality: an increase in access to and use of treatment, which may be due to changes in insurance coverage; and an increase in entry of dialysis clinics and transplant programs.\n\n\n\nPhysician Preferences for Aggressive Treatment at the End of Life and Area-Level Health Care Spending: The Johns Hopkins Precursors Study (2017)\nAuthors: Joseph J. Gallo, Martin S. Andersen, Seungyoung Hwang, Lucy Meoni, Ravishankar Jayadevappa\nRead the paper\n\n\n\nAbstract\n\nObjective: To determine whether physician preferences for end-of-life care were associated with variation in health care spending. Method: We studied 737 physicians who completed the life-sustaining treatment questionnaire in 1999 and were linked to end-of-life care data for the years 1999 to 2009 from Medicare-eligible beneficiaries from the Dartmouth Atlas of Health Care (in hospital-related regions [HRRs]). Using latent class analysis to group physician preferences for end-of-life treatment into most, intermediate, and least aggressive categories, we examined how physician preferences were associated with health care spending over a 7-year period. Results: When all HRRs in the nation were arrayed in quartiles by spending, the prevalence of study physicians who preferred aggressive end-of-life care was greater in the highest spending HRRs. The mean area-level intensive care unit charges per patient were estimated to be US$1,595 higher in the last 6 months of life and US$657 higher during the hospitalization in which death occurred for physicians who preferred the most aggressive treatment at the end of life, when compared with average spending. Conclusions: Physician preference for aggressive end-of-life care was correlated with area-level spending in the last 6 months of life. Policy measures intended to minimize geographic variation in health care spending should incorporate physician preferences and style.\n\n\n\nEffect of Prescription Drug Coupons on Statin Utilization and Expenditures: A Retrospective Cohort Study (2017)\nAuthors: Matthew Daubresse, Martin Andersen, Kevin R. Riggs, G. Caleb Alexander\nRead the paper\n\n\n\nAbstract\n\nImportance Drug coupons are widely used, but their effects are not well understood. Objective To quantify the effect of coupons on statin use and expenditures. Design Retrospective cohort analysis of IMS Health LRx LifeLink database. Setting U.S. retail pharmacy transactions. Participants Incident statin users who initiated branded atorvastatin or rosuvastatin between June 2006 and February 2013. Main Outcomes and Measures Monthly statin utilization (pill-days of therapy), switching (filling a different statin), termination (failure to refill statin for 6 mo), and out-of-pocket and total costs. Results Of 1.1 million incident atorvastatin and rosuvastatin users, 2% used a coupon for at least one statin fill. At 1 year, compared with noncoupon users, those who used a statin coupon on their first fill were dispensed an equal number of monthly pill-days (23.7 vs 23.8), were less likely to switch statins (14.4% vs 16.3%), and were less likely to have terminated statin therapy (31.3% vs 39.2%). At 4 years, coupon users were more likely to have switched (45.5% vs 40.8%) and less likely to have terminated statin therapy (50.6% vs 61.1%) compared with noncoupon users. Those who used greater numbers of coupons were substantially less likely to switch and terminate statin therapies. Monthly out-of-pocket costs were lower among coupon than noncoupon users at 1 year ($9.7 vs $15.1), but total monthly costs were qualitatively similar ($115.5 vs $116.9). At 4 years, monthly out-of-pocket costs among coupon users remained lower ($14.3 vs $16.6) compared with noncoupon users. Sensitivity analyses supported the main results. Conclusions Coupons for branded statins are associated with higher utilization and lower rates of discontinuation and short-term switching to other statin products.\n\n\n\nThe Share Price Effect of CVS Health’s Announcement to Stop Selling Tobacco: A Comparative Case Study Using Synthetic Controls (2017)\nAuthors: Martin Andersen, Sebastian Bauhoff\nRead the paper\n\n\n\nAbstract\n\nWe study how the announcement by CVS Health, a large US-based pharmacy chain, to stop selling tobacco products affected its share price and that of its close competitors, as well as major tobacco companies. Combining event study and synthetic control methodologies we compare measures of CVS’s stock market valuation with those of a peer group consisting of large publicly listed firms that are part of Standard & Poor’s S&P 500 stock market index. CVS’s announcement is associated with a short-term decrease in its share price, whereas close competitors have benefitted from CVS’ decision. We also find a negative share price effect for Altria, the largest US domestic tobacco firm. Overall our findings are consistent with markets expecting consumers to shift from CVS to alternative outlets in the short-run, and interpreting CVS’ decision to drop tobacco products as signal that other firms may follow suit.\n\n\n\nOn the failure of scientific research: an analysis of SBIR projects funded by the U.S. National Institutes of Health (2017)\nAuthors: Martin S. Andersen, Jeremy W. Bray, Albert N. Link\nRead the paper\n\n\n\nAbstract\n\nThe Small Business Innovation Research (SBIR) program is the primary source of public funding in the United States for research by small firms on new technologies, and the National Institutes of Health (NIH) is a major contributor to that funding agenda. Although previous research has explored the determinants of research success for NIH SBIR projects, little is known about the determinants of project failure. This paper provides important, new evidence on the characteristics of NIH SBIR projects that fail. Specifically, we find that firms that have a founder with a business background are less likely to have their funded projects fail. We also find, after controlling for the endogenous nature of woman-owned firms, that such firms are also less likely to fail.\n\n\n\nConstraints on Formulary Design Under the Affordable Care Act (2017)\nAuthors: Martin Andersen\nRead the paper\n\n\n\nAbstract\n\nI study the effect of prescription drug essential health benefits (EHB) requirements from the Affordable Care Act on prescription drug formularies of health insurance marketplace plans. The EHB regulates the number of drugs covered but leaves other dimensions (cost sharing and utilization management) of the formulary unregulated. Using data on almost all formularies in the country, I demonstrate that requiring insurers to cover one additional drug adds 0.22 drugs (3.3%) to the average formulary, mostly owing to firms increasing the number of drugs covered to comply with the EHB requirement. The EHB requirement also increases the probability that a drug is subject to utilization management and is assigned to a higher (more costly) formulary tier. My results suggest that newly covered drugs are 22.3 percentage points more likely to be subject to utilization management, compared to 36.7% for the average covered drug. Using formularies for Medicare Advantage plans, which are subject to uniform, nationwide benefit design standards, and the formulary status of newly approved drugs that do not satisfy the EHB requirement, I reject the hypotheses that consumer demand or effects on plan entry can explain my results. Copyright © 2017 John Wiley & Sons, Ltd.\n\n\n\nImpact of HIE Drug Formularies on Patient Out-of-Pocket Costs (2016)\nAuthors: Martin S. Andersen, Christine Buttorff, G. Caleb Alexander\nRead the paper\n\n\n\nAbstract\n\nABSTRACT Objectives: To assess the generosity of drug coverage in federally facilitated Health Insurance Exchanges (HIEs). Study Design: We linked publicly available benefit design data from 36 federally facilitated HIEs with drug formulary data for 10 therapeutic categories from Managed Markets Insight & Technology. We used health plans as our unit of analysis. Methods: We created a generosity index reflecting the ratio of patients’ out-of-pocket payments to the total drug price (higher values indicate less generous coverage). Because patients’ total out-of-pocket spending changes throughout the year due to deductibles and out-of-pocket maximums, we also examined generosity by varying levels of annual drug spending. Results: Platinum plans covered nearly 90% of drugs that we studied, while bronze, silver, and gold plans covered approximately 80%; results by therapeutic category were similar. Nine in 10 plans used 4- to 6-tier drug formularies, and approximately 24% of branded drugs were associated with some type of utilization management. Bronze plans were less generous than other metal levels, requiring consumers to pay about 94% of the total pharmaceutical costs prior to the deductible being met, while platinum plans required consumers to pay about 43% of such costs. As consumers’ out-of-pocket spending increased throughout the year, differences in generosity across plans narrowed. Conclusions: Benefit structure and formulary design contribute to significant variation in prescription drug coverage generosity within HIEs. These characteristics of health plans are important for individuals with high out-of-pocket costs.\n\n\n\nModernizing Medicare’s Benefit Design and Low-Income Subsidies to Ensure Access and Affordability. (2015)\nAuthors: C. Schoen, K. Davis, C. Buttorff, M. Andersen\nRead the paper\n\n\n\nAbstract\n\nInsurance coverage through the traditional Medicare program is complex, fragmented, and incomplete. Beneficiaries must purchase supplemental private insurance to fill in the gaps. While impoverished beneficiaries may receive supplemental coverage through Medicaid and subsidies for prescription drugs, help is limited for people with incomes above the poverty level. This patchwork quilt leads to confusion for beneficiaries and high administrative costs, while also undermining coverage and care coordination. Most important, Medicare’s benefits fail to limit out-of-pocket costs or ensure adequate financial protection, especially for beneficiaries with low incomes and serious health problems. This brief, part of a series about Medicare’s past, present, and future, presents options for an integrated benefit for enrollees in traditional Medicare. The new benefit would not only reduce cost burdens but also could potentially strengthen the Medicare program and enhance its role in stimulating and supporting innovations throughout the health care delivery system.\n\n\n\nComparing Employer-Sponsored And Federal Exchange Plans: Wide Variations In Cost Sharing For Prescription Drugs (2015)\nAuthors: Christine Buttorff, Martin S. Andersen, Kevin R. Riggs, G. Caleb Alexander\nRead the paper\n\n\n\nAbstract\n\nJust under seven million Americans acquired private insurance through the new health insurance exchanges, or Marketplaces, in 2014. The exchange plans are required to cover essential health benefits, including prescription drugs. However, the generosity of prescription drug coverage in the plans has not been well described. Our primary objective was to examine the variability in drug coverage in the exchanges across plan types (health maintenance organization or preferred provider organization) and metal tiers (bronze, silver, gold, and platinum). Our secondary objective was to compare the exchange coverage to employer-sponsored coverage. Analyzing prescription drug benefit design data for the federally facilitated exchanges, we found wide variation in enrollees’ out-of-pocket costs for generic, preferred brand-name, nonpreferred brand-name, and specialty drugs, not only across metal tiers but also within those tiers across plan types. Compared to employer-sponsored plans, exchange plans generally had lower premiums but provided less generous drug coverage. However, for low-income enrollees who are eligible for cost-sharing subsidies, the exchange plans may be more comparable to employer-based coverage. Policies and programs to assist consumers in matching their prescription drug needs with a plan’s benefit design may improve the financial protection for the newly insured.\n\n\n\nPolicy Options To Expand Medicare’s Low-Income Provisions To Improve Access And Affordability (2015)\nAuthors: Cathy Schoen, Christine Buttorff, Martin Andersen, Karen Davis\nRead the paper\n\n\n\nAbstract\n\nFor fifty years Medicare has enhanced the health and financial security of seniors. Yet in 2014 an estimated 40 percent of low-income beneficiaries spent 20 percent or more of their incomes on out-of-pocket expenditures for premiums and medical care, while one-third were underinsured based on their out-of-pocket spending for medical care alone. These high burdens reflect Medicare’s limited benefits and restrictive income eligibility levels for supplemental Medicaid coverage. We examined the impacts of illustrative policies designed to improve beneficiaries’ financial protection and access to care by reducing Medicare premiums and cost sharing for covered benefits on a sliding scale for all beneficiaries with incomes up to 200 percent of the federal poverty level. We estimate that these policies could improve the affordability of health care for eleven million people. Designed to be aligned with the Affordable Care Act’s subsidy approach for the population younger than age sixty-five, these policies also have the potential to smooth transitions into Medicare, reduce administrative costs, and provide a more secure and equitable foundation for Medicare’s future.\n\n\n\nHeterogeneity and the effect of mental health parity mandates on the labor market (2015)\nAuthors: Martin S. Andersen\nRead the paper\n\n\n\nAbstract\n\nHealth insurance benefit mandates are believed to have adverse effects on the labor market, but efforts to document such effects for mental health parity mandates have had limited success. I show that one reason for this failure is that the association between parity mandates and labor market outcomes vary with mental distress. Accounting for this heterogeneity, I find adverse labor market effects for non-distressed individuals, but favorable effects for moderately distressed individuals and individuals with a moderately distressed family member. On net, I conclude that the mandates are welfare increasing for moderately distressed workers and their families, but may be welfare decreasing for non-distressed individuals.\n\n\n\nPredictors of Venous Thromboembolism in Patients with Advanced Common Solid Cancers (2009)\nAuthors: Isaac E. Hall, Martin S. Andersen, Harlan M. Krumholz, Cary P. Gross\nRead the paper\n\n\n\nAbstract\n\nThere is uncertainty about risk heterogeneity for venous thromboembolism (VTE) in older patients with advanced cancer and whether patients can be stratified according to VTE risk. We performed a retrospective cohort study of the linked Medicare-Surveillance, Epidemiology, and End Results cancer registry in older patients with advanced cancer of lung, breast, colon, prostate, or pancreas diagnosed between 1995–1999. We used survival analysis with demographics, comorbidities, and tumor characteristics/treatment as independent variables. Outcome was VTE diagnosed at least one month after cancer diagnosis. VTE rate was highest in the first year (3.4%). Compared to prostate cancer (1.4 VTEs/100 person-years), there was marked variability in VTE risk (hazard ratio (HR) for male-colon cancer 3.73 (95% CI 2.1–6.62), female-colon cancer HR 6.6 (3.83–11.38), up to female-pancreas cancer HR 21.57 (12.21–38.09). Stage IV cancer and chemotherapy resulted in higher risk (HRs 1.75 (1.44–2.12) and 1.31 (1.0–1.57), resp.). Stratifying the cohort by cancer type and stage using recursive partitioning analysis yielded five groups of VTE rates (nonlocalized prostate cancer 1.4 VTEs/100 person-years, to nonlocalized pancreatic cancer 17.4 VTEs/100 patient-years). In a high-risk population with advanced cancer, substantial variability in VTE risk exists, with notable differences according to cancer type and stage.\n\n\n\nRacial disparities in cancer therapy (2008)\nAuthors: Cary P. Gross, Benjamin D. Smith, Elizabeth Wolf, Martin Andersen\nRead the paper\n\n\n\nAbstract\n\nThe purpose of this study was to determine whether racial disparities in cancer therapy had diminished since the time they were initially documented in the early 1990s.The authors identified a cohort of patients in the SEER-Medicare linked database who were ages 66 to 85 years and who had a primary diagnosis of colorectal, breast, lung, or prostate cancer during 1992 through 2002. The authors identified 7 stage-specific processes of cancer therapy by using Medicare claims. Candidate covariates in multivariate logistic regression included year, clinical, and sociodemographic characteristics, and physician access before cancer diagnosis.During the full study period, black patients were significantly less likely than white patients to receive therapy for cancers of the lung (surgical resection of early stage, 64.0% vs 78.5% for blacks and whites, respectively), breast (radiation after lumpectomy, 77.8% vs 85.8%), colon (adjuvant therapy for stage III, 52.1% vs 64.1%), and prostate (definitive therapy for early stage, 72.4% vs 77.2%, respectively). For both black and white patients, there was little or no improvement in the proportion of patients receiving therapy for most cancer therapies studied, and there was no decrease in the magnitude of any of these racial disparities between 1992 and 2002. Racial disparities persisted even after restricting the analysis to patients who had physician access before their diagnosis.There has been little improvement in either the overall proportion of Medicare beneficiaries receiving cancer therapies or the magnitude of racial disparity. Efforts in the last decade to mitigate cancer therapy disparities appear to have been unsuccessful. Cancer 2008. � 2008 American Cancer Society.\n\n\n\nRelation Between Medicare Screening Reimbursement and Stage at Diagnosis for Older Patients With Colon Cancer (2006)\nAuthors: Cary P. Gross, Martin S. Andersen, Harlan M. Krumholz, Gail J. McAvay, Deborah Proctor, Mary E. Tinetti\nRead the paper\n\n\n\nAbstract\n\nContext Medicare’s reimbursement policy was changed in 1998 to provide coverage for screening colonoscopies for patients with increased colon cancer risk, and expanded further in 2001 to cover screening colonoscopies for all individuals. Objective To determine whether the Medicare reimbursement policy changes were associated with an increase in either colonoscopy use or early stage colon cancer diagnosis. Design, Setting, and Participants Patients in the Surveillance, Epidemiology, and End Results Medicare linked database who were 67 years of age and older and had a primary diagnosis of colon cancer during 1992-2002, as well as a group of Medicare beneficiaries who resided in Surveillance, Epidemiology, and End Results areas but who were not diagnosed with cancer. Main Outcome Measures Trends in colonoscopy and sigmoidoscopy use among Medicare beneficiaries without cancer were assessed using multivariate Poisson regression. Among the patients with cancer, stage was classified as early (stage I) vs all other (stages II-IV). Time was categorized as period 1 (no screening coverage, 1992-1997), period 2 (limited coverage, January 1998-June 2001), and period 3 (universal coverage, July 2001-December 2002). A multivariate logistic regression (outcome = early stage) was used to assess temporal trends in stage at diagnosis; an interaction term between tumor site and time was included. Results Colonoscopy use increased from an average rate of 285/100 000 per quarter in period 1 to 889 and 1919/100 000 per quarter in periods 2 (P{}.001) and 3 (P vs 2{}.001), respectively. During the study period, 44 924 eligible patients were diagnosed with colorectal cancer. The proportion of patients diagnosed at an early stage increased from 22.5% in period 1 to 25.5% in period 2 and 26.3% in period 3 (P{}.001 for each pairwise comparison). The changes in Medicare coverage were strongly associated with early stage at diagnosis for patients with proximal colon lesions (adjusted relative risk period 2 vs 1, 1.19; 95% confidence interval, 1.13-1.26; adjusted relative risk period 3 vs 2, 1.10; 95% confidence interval, 1.02-1.17) but weakly associated, if at all, for patients with distal colon lesions (adjusted relative risk period 2 vs 1, 1.07; 95% confidence interval, 1.01-1.13; adjusted relative risk period 3 vs 2, 0.97; 95% confidence interval, 0.90-1.05). Conclusions Expansion of Medicare reimbursement to cover colon cancer screening was associated with an increased use of colonoscopy for Medicare beneficiaries, and for those who were diagnosed with colon cancer, an increased probability of being diagnosed at an early stage. The selective effect of the coverage change on proximal colon lesions suggests that increased use of whole-colon screening modalities such as colonoscopy may have played a pivotal role.\n\n\n\n\nWorking papers\n\nEstimating the Impact of Temporary COVID-19 College Closures on the 2020 Census Count (2023)\nAuthors: Martin Andersen, Emefa Buaka, D. Sunshine Hillygus, Christopher R. Marsicano, Rylie C. Martin\nRead the paper\n\n\n\nAbstract\n\nTemporary college closures in response to the COVID-19 pandemic created an exodus of students from college towns just as the decennial census count was getting underway. We use aggregate cellular mobility data to evaluate if this population movement affected the distributional accuracy of the 2020 Census. Based on the outflow of devices in late March 2020, we estimate that counties with a college were undercounted by two percent, likely affecting Congressional apportionment.\n\n\n\nCOVID-19 Restrictions Reduced Abortion Clinic Visits, Even in Blue States (2020)\nAuthors: Martin Andersen, Sylvia Bryan, David Slusky\nRead the paper\n\n\n\nAbstract\n\nFounded in 1920, the NBER is a private, non-profit, non-partisan organization dedicated to conducting economic research and to disseminating research findings among academics, public policy makers, and business professionals.\n\n\n\nEarly Evidence on Social Distancing in Response to COVID-19 in the United States (2020)\nAuthors: Martin Andersen\nRead the paper\n\n\n\nAbstract\n\nThe COVID-19 pandemic threatens to overwhelm the US health care system if unchecked. Social distancing measures, which may slow the spread of infectious disease, may allow the US health care system time to expand and prepare to respond to COVID-19. I demonstrate that there has been substantial voluntary social distancing and provide some evidence that mandatory measures have also been effective at reducing the frequency of visits to locations outside of one’s home. However, voluntary social distancing is moderated by partisanship and media consumptions in ways that heighten the importance of honest, clear, and consistent communications by political leaders and the media.\n\n\n\n\nActive projects"
  },
  {
    "objectID": "enrollment_trends.html#enrollment-trends-national-data",
    "href": "enrollment_trends.html#enrollment-trends-national-data",
    "title": "Untitled",
    "section": "Enrollment trends (national data)",
    "text": "Enrollment trends (national data)"
  },
  {
    "objectID": "teaching_resources/visualizations.html",
    "href": "teaching_resources/visualizations.html",
    "title": "Reading in data and basic data manipulation using R",
    "section": "",
    "text": "In this page we will build on our work in the previous page, where we learned how to read-in data and conduct some basic data manipulation operations."
  },
  {
    "objectID": "teaching_resources/visualizations.html#getting-started",
    "href": "teaching_resources/visualizations.html#getting-started",
    "title": "Reading in data and basic data manipulation using R",
    "section": "",
    "text": "In this page we will build on our work in the previous page, where we learned how to read-in data and conduct some basic data manipulation operations."
  },
  {
    "objectID": "teaching_resources/visualizations.html#getting-back-to-where-we-were",
    "href": "teaching_resources/visualizations.html#getting-back-to-where-we-were",
    "title": "Reading in data and basic data manipulation using R",
    "section": "Getting back to where we were",
    "text": "Getting back to where we were\nLast time, we saved our work in a file called “nhis_working_data.csv” in a directory that you chose (or the default directory). We will read in that file using the same read_csv command that we used to download the original data. Before we can do that, however, we need to acitvate the tidyverse packages:\n\nlibrary(tidyverse) data&lt;-read_csv(“nhis_working_data.csv”)\n\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nRows: 4128 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): sex\ndbl (10): year, age, pop, any_insurance, private_insurance, chip_insurance, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe can see from the output that we have attached the tidyverse packages and opened the NHIS file that we were working on last time. To double check, make sure that you have the variables over65 and age_minus_65 in data."
  },
  {
    "objectID": "teaching_resources/visualizations.html#visualizaing-data",
    "href": "teaching_resources/visualizations.html#visualizaing-data",
    "title": "Reading in data and basic data manipulation using R",
    "section": "Visualizaing data",
    "text": "Visualizaing data\n\nPlotting point-based data\nWe can use the ggplot function to plot data. We will start by plotting the relationship between age and insurance coverage. ggplot uses a set of ideas that can be roughly classified as data, aesthetics, and geometries. We feed in the data to ggplot, tell it which variables correspond to which aesthetic elements, and use a geometry to plot the data. For example, for “point” or “scatter” plots, we use the geom_point geometry which requires the aesthetics x and y. Here’s the code to plot the age pattern of insurance coverage:\n\ndata|&gt;ggplot(aes(x=age, y=any_insurance))+geom_point()\n\nIn this code, ggplot says that we are creating a new plot; the aes function let’s us define our aesthetics, and geom_point says that we are plotting point data. The x aesthetic is the age variable and the y aesthetic is the any_insurance variable. The plot looks like this:\n\n\n\n\n\n\n\n\n\nThere are a lot of points there! That is because data contains an observation for each age-year-sex cell; let’s summarize our data to the age level (remember that pop holds the size of each cell).\n\ndata|&gt;group_by(age)|&gt;summarize(any_insurance=weighted.mean(any_insurance, pop))|&gt;ggplot(aes(x=age, y=any_insurance))+geom_point()\n\n\n\n\n\n\n\n\n\n\nThis graph is much easier to read and demonstrates that insurance coverage is higher among older individuals and children than people in the “middle” age range.\n\n\nPlotting line-based data\nIn order to dive into the patterns we are seeing in the insurance coverage data in more detail, we will first switch to plotting these data as lines charts rather than scatter plots. We can make this switch by replaceing geom_point with geom_line:\n\ndata|&gt;group_by(age)|&gt;summarize(any_insurance=weighted.mean(any_insurance, pop))|&gt;ggplot(aes(x=age, y=any_insurance))+geom_line()\n\n\n\n\n\n\n\n\n\n\nThis line chart shows the same basic pattern as the scatter plot. Now we will make more sophisticed use of geom_line to help us see what is happening in insurance coverage by age. Specifically, we do not have to specify all of the aesthetics in ggplot; we can also specify them in the geom_line command AND we can combine several of them together using the + sign. Let’s plot any_insurance, mcare_insurance, and private_insurance first (note the use of across to get the weighted mean for each insurance type):\n\ndata|&gt;group_by(age)|&gt;summarize(across(c(any_insurance,mcare_insurance,private_insurance), ~weighted.mean(., pop)))|&gt;ggplot(aes(x=age))+geom_line(aes(y=any_insurance))\n\n\n\n\n\n\n\n\n\n\nHmm… this doesn’t look so easy to read because all of the lines look identical. We can fix that by telling geom_line that there is an aesthetic color that will take on various values.\n\ndata|&gt;group_by(age)|&gt;summarize(across(c(any_insurance,mcare_insurance,private_insurance), ~weighted.mean(., pop)))|&gt;ggplot(aes(x=age))+geom_line(aes(y=any_insurance,color=“Any”))+geom_line(aes(y=mcare_insurance, color=“Medicare”))+geom_line(aes(y=private_insurance,color=“Private”))\n\n\n\n\n\n\n\n\n\n\nNow we can see that the increase in insurance coverage at age 65 is from Medicare coverage, which rises from essentially 0% to almost 100% after people turn 65. However, this doesn’t explain what is happening among kids. Let’s expand the insurance types we are considering to include CHIP and Medicaid too:\n\ndata|&gt;group_by(age)|&gt;summarize(across(c(any_insurance,mcare_insurance,private_insurance), ~weighted.mean(., pop)))|&gt;ggplot(aes(x=age))+geom_line(aes(y=any_insurance,color=“Any”))+geom_line(aes(y=mcare_insurance, color=“Medicare”))+geom_line(aes(y=private_insurance,color=“Private”))\n\n\n\n\n\n\n\n\n\n\nThis graph demonstrates that the decline in insurance coverage around 18 years of age comes from reductions in Medicaid and, to a lesser extent, CHIP coverage.\nLooking at the line plots, it appears that there are three main groups that we would like to think about: children, adults, and the elderly. Let’s define a variable, age_group that assigns each cell to one of these three groups. We will use a new command, case_when to do this. case_when is a function that allows us to create new variables based on a series of conditions. Here is the code to create the age_group variable:\n\ndata|&gt;mutate(age_groups=case_when(age&lt;=18“Children”,age&lt;=64“Adults”,TRUE~“Elderly”))\n\nRemember to assign the results back to data!\nCheck your work by counting these levels:\n\n\n# A tibble: 3 × 2\n  age_groups     n\n  &lt;chr&gt;      &lt;int&gt;\n1 Adults      2208\n2 Children     912\n3 Elderly     1008\n\n\nNow that we have the age_group variable, we can use it to plot the insurance coverage data by age group over time. We will use year as the x aesthetic and age_group as the color aesthetic.\n\ndata|&gt;group_by(age_groups,year)|&gt;summarize(across(c(any_insurance,mcare_insurance,private_insurance), ~weighted.mean(., pop)))|&gt;ggplot(aes(x=year, group=age_group))+geom_line(aes(y=any_insurance)\n\n\n\n`summarise()` has grouped output by 'age_groups'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nThis graph shows that insurance coverage has been increasing for children throughout this time period and that there was a dramatic increase in insurance coverage for adults in 2014.\n\n\nPlotting with multiple panels\nWe can use the facet_wrap function to create multiple panels in a single plot. This function takes a formula that specifies the variable that we want to use to create the panels. We will make a plot that shows each of our age groups and allows us to see how insurance coverage has changed over time for each type of coverage.\n\ndata|&gt;group_by(age_groups,year)|&gt;summarize(across(c(any_insurance,mcare_insurance,private_insurance,chip_insurance,mcaid_insurance), ~weighted.mean(., pop)))|&gt;ggplot(aes(x=year))+geom_line(aes(y=any_insurance,color=“Any”))+geom_line(aes(y=mcare_insurance, color=“Medicare”))+geom_line(aes(y=private_insurance,color=“Private”))+geom_line(aes(y=chip_insurance,color=“CHIP”))+geom_line(aes(y=mcaid_insurance,color=“Medicaid”))+facet_wrap(~age_groups)\n\n\n\n`summarise()` has grouped output by 'age_groups'. You can override using the\n`.groups` argument."
  },
  {
    "objectID": "teaching_resources/visualizations.html#practice",
    "href": "teaching_resources/visualizations.html#practice",
    "title": "Reading in data and basic data manipulation using R",
    "section": "Practice",
    "text": "Practice\nHere are some exercises that you can use as practice to help you make sure that you have understood the concepts. In some cases you will need to look at the helpfiles. You can do this by typing ?function_name in the console.\n\nCreate a line plot that shows the relationship between age and the proportion of people with insurance coverage before and after the ACA was implemented. Please use facet_wrap to create a two panel graph and show trends for each of the listed insurance types.\nLook at the help for geom_bar and geom_col. Choose the appropriate function (either will work, but will require different setup steps). Create a bar plot that shows the proportion of people with each insurance type by before vs. after the ACA, our three age groups, and gender."
  }
]